{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some colours\n",
    "LIGHT_RED = '#FFC4CC'\n",
    "LIGHT_GREEN = '#95FD99'\n",
    "BLACK = '#000000'\n",
    "WHITE = '#FFFFFF'\n",
    "LIGHT_PURPLE = '#E8D0FF'\n",
    "LIGHT_ORANGE = '#FAE0C3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    # Actions\n",
    "    STAY = 0\n",
    "    MOVE_LEFT = 1\n",
    "    MOVE_RIGHT = 2\n",
    "    MOVE_UP = 3\n",
    "    MOVE_DOWN = 4\n",
    "\n",
    "    # Give names to actions\n",
    "    actions_names = {\n",
    "        STAY: \"stay\",\n",
    "        MOVE_LEFT: \"move left\",\n",
    "        MOVE_RIGHT: \"move right\",\n",
    "        MOVE_UP: \"move up\",\n",
    "        MOVE_DOWN: \"move down\"\n",
    "    }\n",
    "\n",
    "    # Reward values\n",
    "    STEP_REWARD = -1\n",
    "    GOAL_REWARD = 0\n",
    "    IMPOSSIBLE_REWARD = -100\n",
    "    DEAD_REWARD = -100\n",
    "    \n",
    "    def __init__(self, maze, weights=None, random_rewards=False):\n",
    "        \"\"\" Constructor of the environment Maze.\n",
    "        \"\"\"\n",
    "        self.maze = maze\n",
    "        self.actions = self.__actions()\n",
    "        self.states, self.map = self.__states()\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = len(self.states)\n",
    "        self.transition_probabilities = self.__transitions()\n",
    "        self.rewards = self.__rewards(weights=weights)\n",
    "        \n",
    "    def __actions(self):\n",
    "        actions = dict()\n",
    "        actions[self.STAY] = (0, 0)\n",
    "        actions[self.MOVE_LEFT] = (0, -1)\n",
    "        actions[self.MOVE_RIGHT] = (0, 1)\n",
    "        actions[self.MOVE_UP] = (-1, 0)\n",
    "        actions[self.MOVE_DOWN] = (1, 0)\n",
    "        return actions\n",
    "\n",
    "    def __states(self):\n",
    "        states = dict()\n",
    "        map = dict()\n",
    "        s = 0\n",
    "\n",
    "        for i in range(self.maze.shape[0]):\n",
    "            for j in range(self.maze.shape[1]):\n",
    "                for i_m in range(self.maze.shape[0]):\n",
    "                    for j_m in range(self.maze.shape[1]):\n",
    "                        if (self.maze[i, j] != 1) & (self.maze[i_m, j_m] != 1):\n",
    "                            states[s] = (i, j, i_m, j_m)\n",
    "                            map[(i, j, i_m, j_m)] = s\n",
    "                            s += 1\n",
    "        return states, map\n",
    "    \n",
    "    def __move(self, state, action, p):\n",
    "        row = self.states[state][0] + self.actions[action][0]\n",
    "        col = self.states[state][1] + self.actions[action][1]\n",
    "\n",
    "        hitting_maze_walls = (row == -1) or (row == self.maze.shape[0]) or \\\n",
    "                             (col == -1) or (col == self.maze.shape[1]) or \\\n",
    "                             (self.maze[row, col] == 1)\n",
    "        if hitting_maze_walls:\n",
    "            return self.map[(self.states[state][0], self.states[state][1], p[0], p[1])]\n",
    "        else:             \n",
    "            return self.map[(row, col, p[0], p[1])]\n",
    "\n",
    "    def __possible_pos_minotaur(self, state):\n",
    "        pos = list()\n",
    "        for action in self.actions:\n",
    "            if action == 0:\n",
    "                continue\n",
    "            row = self.states[state][2] + self.actions[action][0]\n",
    "            col = self.states[state][3] + self.actions[action][1]\n",
    "            if (row == -1) or (row == self.maze.shape[0]) or (col == -1) or (col == self.maze.shape[1]):\n",
    "                continue\n",
    "            elif self.maze[row,col] == 1:\n",
    "                if self.maze[row + self.actions[action][0], col + self.actions[action][1]] == 0:\n",
    "                    pos.append((row + self.actions[action][0], col + self.actions[action][1]))\n",
    "            else:\n",
    "                pos.append((row, col))                \n",
    "        return pos\n",
    "        \n",
    "    \n",
    "    def __transitions(self):\n",
    "        dimensions = (self.n_states, self.n_states, self.n_actions)\n",
    "        transition_probabilities = np.zeros(dimensions)\n",
    "        \n",
    "        for s in range(self.n_states): \n",
    "            pos_m = self.__possible_pos_minotaur(s)           \n",
    "            for a in range(self.n_actions):                \n",
    "                for p in pos_m:\n",
    "                    next_s = self.__move(s, a, p)\n",
    "                    transition_probabilities[next_s, s, a] = 1/len(pos_m)\n",
    "        return transition_probabilities\n",
    "\n",
    "\n",
    "    def __rewards(self, weights=None):\n",
    "        rewards = np.zeros((self.n_states, self.n_actions))\n",
    "        # If the rewards are not described by a weight matrix\n",
    "        if weights is None:\n",
    "            for s in range(self.n_states):                \n",
    "                for a in range(self.n_actions):\n",
    "                    pos = self.__possible_pos_minotaur(s)                     \n",
    "                    for p in pos:\n",
    "                        next_s = self.__move(s, a, p)\n",
    "                        # Reward for hitting a wall\n",
    "                        if self.states[s][0:2] == self.states[next_s][0:2] and a != self.STAY:\n",
    "                            rewards[s, a] += self.IMPOSSIBLE_REWARD\n",
    "                        # Reward for reaching the exit\n",
    "                        elif self.states[s][0:2] == self.states[next_s][0:2] and self.maze[self.states[next_s][0:2]] == 2:\n",
    "                            rewards[s, a] += self.GOAL_REWARD\n",
    "                        # Reward if the minotaur eats the user\n",
    "                        elif (self.states[s][0] == self.states[s][2]) and (self.states[s][1] == self.states[s][3]):\n",
    "                            rewards[s, a] += self.DEAD_REWARD\n",
    "                        # Reward for taking a step to an empty cell that is not the exit\n",
    "                        else:\n",
    "                            rewards[s, a] += self.STEP_REWARD\n",
    "                            \n",
    "                    rewards[s, a] /= len(pos)\n",
    "        # If the weights are described by a weight matrix\n",
    "        else:\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    next_s = self.__move(s, a)\n",
    "                    i, j = self.states[next_s]\n",
    "                    # Simply put the reward as the weights o the next state.\n",
    "                    rewards[s, a] = weights[i][j]\n",
    "        return rewards\n",
    "\n",
    "\n",
    "    def simulate(self, start, policy, method):\n",
    "        path = list()\n",
    "        \n",
    "        if method == 'DynProg':\n",
    "            # Deduce the horizon from the policy shape\n",
    "            horizon = policy.shape[1]\n",
    "            # Initialize current state and time\n",
    "            t = 0\n",
    "            s = self.map[start]\n",
    "            # Add the starting position in the maze to the path\n",
    "            path.append(start)            \n",
    "            while t < horizon - 1:\n",
    "                # Move to next state given the policy and the current state\n",
    "                 \n",
    "                pos = self.__possible_pos_minotaur(s)\n",
    "\n",
    "                next_s = self.__move(s, policy[s, t], random.choice(pos))\n",
    "                # Add the position in the maze corresponding to the next state\n",
    "                # to the path'numpy.ndarray' object is not callable\n",
    "                path.append(self.states[next_s])\n",
    "                # Update time and state for next iteration\n",
    "                t += 1\n",
    "                s = next_s\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def show(self):\n",
    "        print('The states are :')\n",
    "        print(self.states)\n",
    "        print('The actions are:')\n",
    "        print(self.actions)\n",
    "        print('The mapping of the states:')\n",
    "        print(self.map)\n",
    "        print('The rewards:')\n",
    "        print(self.rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "\n",
    "env = Maze(maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming(env, horizon):\n",
    "    p = env.transition_probabilities\n",
    "    r = env.rewards\n",
    "    T = horizon\n",
    "    n_states = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    \n",
    "    # The variables involved in the dynamic programming backwards recursions\n",
    "    V = np.zeros((n_states, T + 1))\n",
    "    policy = np.zeros((n_states, T + 1))\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    print(r.shape)\n",
    "    #Initialization\n",
    "    Q = np.copy(r)\n",
    "    V[:, T] = np.max(Q, 1)\n",
    "    policy[:, T] = np.argmax(Q, 1)\n",
    "    \n",
    "    # The dynamic programming backwards recursion\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s, a] = r[s, a] + np.dot(p[:, s, a], V[:, t + 1])\n",
    "                \n",
    "        # Update by taking the maximum Q value w.r.t the action a\n",
    "        V[:, t] = np.max(Q, 1)\n",
    "        policy[:, t] = np.argmax(Q, 1)    \n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "    \n",
    "def value_iteration(env, gamma, epsilon):\n",
    "    p = env.transition_probabilities\n",
    "    r = env.rewards\n",
    "    n_states = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    \n",
    "    V = np.zeros(n_states)    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    BV = np.zeros(n_states)\n",
    "    \n",
    "    n = 0\n",
    "    # Tolerance error\n",
    "    tol = (1 - gamma) * epsilon / gamma\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = r[s, a] + gamma * np.dot(p[:, s, a], V)\n",
    "    BV = np.max(Q, 1)            \n",
    "    \n",
    "    # Iterate until convergence\n",
    "    while np.linalg.norm(V - BV) >= tol and n < 200:\n",
    "        # Increment by one the numbers of iteration\n",
    "        n += 1;\n",
    "        # Update the value function\n",
    "        V = np.copy(BV);\n",
    "        # Compute the new BV\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s, a] = r[s, a] + gamma * np.dot(p[:, s, a], V);\n",
    "        BV = np.max(Q, 1);\n",
    "        # Show error\n",
    "        # print(np.linalg.norm(V - BV))\n",
    "\n",
    "    # Compute policy\n",
    "    policy = np.argmax(Q, 1);\n",
    "    # Return the obtained policy\n",
    "    return V, policy;\n",
    "\n",
    "def draw_maze(maze):\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -6: LIGHT_RED, -1: LIGHT_RED}\n",
    "\n",
    "    # Give a color to each cell\n",
    "    rows, cols = maze.shape\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows));\n",
    "\n",
    "    # Remove the axis ticks and add title title\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('The Maze')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Give a color to each cell\n",
    "    rows, cols = maze.shape\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows))\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(cellText=None,\n",
    "                     cellColours=colored_maze,\n",
    "                     cellLoc='center',\n",
    "                     loc=(0, 0),\n",
    "                     edges='closed')\n",
    "    # Modify the height and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0 / rows);\n",
    "        cell.set_width(1.0 / cols);\n",
    "\n",
    "\n",
    "def animate_solution(maze, path):\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -6: LIGHT_RED, -1: LIGHT_RED};\n",
    "\n",
    "    # Size of the maze\n",
    "    rows, cols = maze.shape\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows))\n",
    "\n",
    "    # Remove the axis ticks and add title title\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('Policy simulation')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Give a color to each cell\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)];\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows))\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(cellText=None,\n",
    "                     cellColours=colored_maze,\n",
    "                     cellLoc='center',\n",
    "                     loc=(0, 0),\n",
    "                     edges='closed')\n",
    "    # Modify the hight and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0 / rows)\n",
    "        cell.set_width(1.0 / cols)\n",
    "\n",
    "    # Update the color at each frame PLAYER\n",
    "    for i in range(len(path)):\n",
    "        grid.get_celld()[(path[i][0:2])].set_facecolor(LIGHT_ORANGE)\n",
    "        grid.get_celld()[(path[i][0:2])].get_text().set_text('Player')\n",
    "        \n",
    "        grid.get_celld()[(path[i][2:4])].set_facecolor(LIGHT_ORANGE)\n",
    "        grid.get_celld()[(path[i][2:4])].get_text().set_text('MINOTAUR')\n",
    "        \n",
    "        if i > 0:\n",
    "            grid.get_celld()[(path[i - 1][2:4])].set_facecolor(col_map[maze[path[i - 1][2:4]]])\n",
    "            grid.get_celld()[(path[i - 1][2:4])].get_text().set_text('')\n",
    "            \n",
    "            if path[i][0:2] == path[i - 1][0:2]:\n",
    "                grid.get_celld()[(path[i][0:2])].set_facecolor(LIGHT_GREEN)\n",
    "                grid.get_celld()[(path[i][0:2])].get_text().set_text('Player is out')\n",
    "                \n",
    "            elif path[i][0:2] == path[i][2:4]:\n",
    "                grid.get_celld()[(path[i])].set_facecolor(LIGHT_RED)\n",
    "                grid.get_celld()[(path[i])].get_text().set_text('KILLED')\n",
    "                \n",
    "            else:\n",
    "                grid.get_celld()[(path[i - 1][0:2])].set_facecolor(col_map[maze[path[i - 1][0:2]]])\n",
    "                grid.get_celld()[(path[i - 1][0:2])].get_text().set_text('')\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 5)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [2. 2. 2. ... 2. 2. 0.]\n",
      " [4. 4. 4. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[(0, 0, 6, 5), (1, 0, 6, 3), (2, 0, 6, 2), (2, 1, 6, 1), (3, 1, 4, 1), (4, 1, 4, 2), (4, 2, 4, 1), (4, 3, 3, 1), (4, 4, 3, 3), (4, 5, 2, 3), (4, 6, 2, 1), (4, 7, 1, 1), (5, 7, 1, 0), (6, 7, 1, 1), (6, 6, 2, 1), (6, 5, 1, 1), (6, 5, 2, 1), (6, 5, 1, 1), (6, 5, 2, 1), (6, 5, 2, 0), (6, 5, 2, 1)]\n"
     ]
    }
   ],
   "source": [
    "horizon = 20\n",
    "V, policy = dynamic_programming(env, horizon)\n",
    "print(policy)\n",
    "\n",
    "method = 'DynProg'\n",
    "start  = (0, 0, 6, 5)\n",
    "path = env.simulate(start, policy, method)\n",
    "\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
