{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import random\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some colours\n",
    "LIGHT_RED = '#FFC4CC'\n",
    "LIGHT_GREEN = '#95FD99'\n",
    "BLACK = '#000000'\n",
    "WHITE = '#FFFFFF'\n",
    "LIGHT_PURPLE = '#E8D0FF'\n",
    "LIGHT_ORANGE = '#FAE0C3'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    # Actions\n",
    "    STAY = 0\n",
    "    MOVE_LEFT = 1\n",
    "    MOVE_RIGHT = 2\n",
    "    MOVE_UP = 3\n",
    "    MOVE_DOWN = 4\n",
    "\n",
    "    # Give names to actions\n",
    "    actions_names = {\n",
    "        STAY: \"stay\",\n",
    "        MOVE_LEFT: \"move left\",\n",
    "        MOVE_RIGHT: \"move right\",\n",
    "        MOVE_UP: \"move up\",\n",
    "        MOVE_DOWN: \"move down\"\n",
    "    }\n",
    "\n",
    "    # Reward values\n",
    "    STEP_REWARD = -1\n",
    "    GOAL_REWARD = 0\n",
    "    IMPOSSIBLE_REWARD = -100\n",
    "    \n",
    "    def __init__(self, maze, weights=None, random_rewards=False):\n",
    "        \"\"\" Constructor of the environment Maze.\n",
    "        \"\"\"\n",
    "        self.maze = maze\n",
    "        self.actions = self.__actions()\n",
    "        self.states, self.map = self.__states()\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = len(self.states)\n",
    "        self.transition_probabilities = self.__transitions()\n",
    "        self.rewards = self.__rewards(weights=weights,\n",
    "                                      random_rewards=random_rewards)\n",
    "        \n",
    "    def __actions(self):\n",
    "        actions = dict()\n",
    "        actions[self.STAY] = (0, 0)\n",
    "        actions[self.MOVE_LEFT] = (0, -1)\n",
    "        actions[self.MOVE_RIGHT] = (0, 1)\n",
    "        actions[self.MOVE_UP] = (-1, 0)\n",
    "        actions[self.MOVE_DOWN] = (1, 0)\n",
    "        return actions\n",
    "\n",
    "    def __states(self):\n",
    "        states = dict()\n",
    "        map = dict()\n",
    "        end = False\n",
    "        s = 0\n",
    "        for i in range(self.maze.shape[0]):\n",
    "            for j in range(self.maze.shape[1]):\n",
    "                if self.maze[i, j] != 1:\n",
    "                    states[s] = (i, j)\n",
    "                    map[(i, j)] = s\n",
    "                    s += 1\n",
    "        return states, map\n",
    "    \n",
    "    def __move(self, state, action):\n",
    "        row = self.states[state][0] + self.actions[action][0]\n",
    "        col = self.states[state][1] + self.actions[action][1]\n",
    "        \n",
    "        hitting_maze_walls = (row == -1) or (row == self.maze.shape[0]) or \\\n",
    "                             (col == -1) or (col == self.maze.shape[1]) or \\\n",
    "                             (self.maze[row, col] == 1)\n",
    "        \n",
    "        if hitting_maze_walls:\n",
    "            return state\n",
    "        else:\n",
    "            return self.map[(row, col)]\n",
    "        \n",
    "    \n",
    "    def __transitions(self):\n",
    "        dimensions = (self.n_states, self.n_states, self.n_actions)\n",
    "        transition_probabilities = np.zeros(dimensions)\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                next_s = self.__move(s, a)\n",
    "                transition_probabilities[next_s, s, a] = 1\n",
    "        return transition_probabilities\n",
    "    \n",
    "    def __rewards(self, weights=None, random_rewards=None):\n",
    "        rewards = np.zeros((self.n_states, self.n_actions));\n",
    "\n",
    "        # If the rewards are not described by a weight matrix\n",
    "        if weights is None:\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    next_s = self.__move(s, a);\n",
    "                    # Rewrd for hitting a wall\n",
    "                    if s == next_s and a != self.STAY:\n",
    "                        rewards[s, a] = self.IMPOSSIBLE_REWARD\n",
    "                    # Reward for reaching the exit\n",
    "                    elif s == next_s and self.maze[self.states[next_s]] == 2:\n",
    "                        rewards[s, a] = self.GOAL_REWARD;\n",
    "                    # Reward for taking a step to an empty cell that is not the exit\n",
    "                    else:\n",
    "                        rewards[s, a] = self.STEP_REWARD;\n",
    "\n",
    "                    # If there exists trapped cells with probability 0.5\n",
    "                    if random_rewards and self.maze[self.states[next_s]] < 0:\n",
    "                        row, col = self.states[next_s];\n",
    "                        # With probability 0.5 the reward is\n",
    "                        r1 = (1 + abs(self.maze[row, col])) * rewards[s, a];\n",
    "                        # With probability 0.5 the reward is\n",
    "                        r2 = rewards[s, a];\n",
    "                        # The average reward\n",
    "                        rewards[s, a] = 0.5 * r1 + 0.5 * r2;\n",
    "        # If the weights are descrobed by a weight matrix\n",
    "        else:\n",
    "            for s in range(self.n_states):\n",
    "                for a in range(self.n_actions):\n",
    "                    next_s = self.__move(s, a);\n",
    "                    i, j = self.states[next_s];\n",
    "                    # Simply put the reward as the weights o the next state.\n",
    "                    rewards[s, a] = weights[i][j];\n",
    "\n",
    "        return rewards;\n",
    "                    \n",
    "    \n",
    "    def __move_minotaur(self, state, action):\n",
    "        pdb.set_trace()\n",
    "        row = self.states[state][0] + self.actions[action][0]\n",
    "        col = self.states[state][1] + self.actions[action][1]    \n",
    "    \n",
    "        hitting_maze_walls = (row == -1) or (row == self.maze.shape[0]) or \\\n",
    "                             (col == -1) or (col == self.maze.shape[1])\n",
    "\n",
    "        wall = (self.maze[row, col] == 1)\n",
    "        if wall:\n",
    "            if self.actions[action][0] == 1:  # moves down\n",
    "                row = self.states[state][0] + 2\n",
    "                col = self.states[state][1] + self.actions[action][1]    \n",
    "\n",
    "            elif self.actions[action][0] == -1:  # moves up\n",
    "                row = self.states[state][0] + (-2)\n",
    "                col = self.states[state][1] + self.actions[action][1]    \n",
    "\n",
    "            elif self.actions[action][1] == 1:  # moves right\n",
    "                row = self.states[state][0] + self.actions[action][0]\n",
    "                col = self.states[state][1] + 2\n",
    "\n",
    "            elif self.actions[action][1] == -1:  # moves left        \n",
    "                col = self.states[state][1] + (-2)  \n",
    "\n",
    "        if hitting_maze_walls:\n",
    "            return state\n",
    "        else:\n",
    "            return self.map[(row, col)]\n",
    "    \n",
    "    \n",
    "    def simulate(self, start, policy, method):\n",
    "        path = list();\n",
    "        \n",
    "        if method == 'DynProg':\n",
    "            # Deduce the horizon from the policy shape\n",
    "            horizon = policy.shape[1]\n",
    "            # Initialize current state and time\n",
    "            t = 0;\n",
    "            s = self.map[start]\n",
    "            # Add the starting position in the maze to the path\n",
    "            path.append(start)            \n",
    "            while t < horizon - 1:\n",
    "                # Move to next state given the policy and the current state\n",
    "                next_s = self.__move(s, policy[s, t])\n",
    "                # Add the position in the maze corresponding to the next state\n",
    "                # to the path\n",
    "                path.append(self.states[next_s])\n",
    "                # Update time and state for next iteration\n",
    "                t += 1\n",
    "                s = next_s\n",
    "                \n",
    "        if method == 'random_walk':\n",
    "            horizon = policy.shape[1]\n",
    "            t = 0\n",
    "            s = self.map[start]\n",
    "            while t < horizon - 1:\n",
    "                next_s = self.__move_minotaur(s, random.choice(self.actions))\n",
    "                path.append(self.states[next_s])\n",
    "                t += 1\n",
    "                s = next_s\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def show(self):\n",
    "        print('The states are :')\n",
    "        print(self.states)\n",
    "        print('The actions are:')\n",
    "        print(self.actions)\n",
    "        print('The mapping of the states:')\n",
    "        print(self.map)\n",
    "        print('The rewards:')\n",
    "        print(self.rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming(env, horizon):\n",
    "    p = env.transition_probabilities\n",
    "    r = env.rewards\n",
    "    T = horizon\n",
    "    n_states = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    \n",
    "    # The variables involved in the dynamic programming backwards recursions\n",
    "    V = np.zeros((n_states, T + 1))\n",
    "    policy = np.zeros((n_states, T + 1))\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    #Initialization\n",
    "    Q = np.copy(r)\n",
    "    V[:, T] = np.max(Q, 1)\n",
    "    policy[:, T] = np.argmax(Q, 1)\n",
    "    \n",
    "    # The dynamic programming bakwards recursion\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s, a] = r[s, a] + np.dot(p[:, s, a], V[:, t + 1])\n",
    "                \n",
    "        # Update by taking the maximum Q value w.r.t the action a\n",
    "       # pdb.set_trace()\n",
    "        V[:, t] = np.max(Q, 1)\n",
    "        policy[:, t] = np.argmax(Q, 1)    \n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "    \n",
    "def value_iteration(env, gamma, epsilon):\n",
    "    p = env.transition_probabilities\n",
    "    r = env.rewards\n",
    "    n_states = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    \n",
    "    V = np.zeros(n_states)    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    BV = np.zeros(n_states)\n",
    "    \n",
    "    n = 0\n",
    "    # Tolerance error\n",
    "    tol = (1 - gamma) * epsilon / gamma\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = r[s, a] + gamma * np.dot(p[:, s, a], V)\n",
    "    BV = np.max(Q, 1)            \n",
    "    \n",
    "    # Iterate until convergence\n",
    "    while np.linalg.norm(V - BV) >= tol and n < 200:\n",
    "        # Increment by one the numbers of iteration\n",
    "        n += 1;\n",
    "        # Update the value function\n",
    "        V = np.copy(BV);\n",
    "        # Compute the new BV\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s, a] = r[s, a] + gamma * np.dot(p[:, s, a], V);\n",
    "        BV = np.max(Q, 1);\n",
    "        # Show error\n",
    "        # print(np.linalg.norm(V - BV))\n",
    "\n",
    "    # Compute policy\n",
    "    policy = np.argmax(Q, 1);\n",
    "    # Return the obtained policy\n",
    "    return V, policy;\n",
    "\n",
    "def draw_maze(maze):\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -6: LIGHT_RED, -1: LIGHT_RED}\n",
    "\n",
    "    # Give a color to each cell\n",
    "    rows, cols = maze.shape\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows));\n",
    "\n",
    "    # Remove the axis ticks and add title title\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('The Maze')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Give a color to each cell\n",
    "    rows, cols = maze.shape\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows))\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(cellText=None,\n",
    "                     cellColours=colored_maze,\n",
    "                     cellLoc='center',\n",
    "                     loc=(0, 0),\n",
    "                     edges='closed')\n",
    "    # Modify the height and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0 / rows);\n",
    "        cell.set_width(1.0 / cols);\n",
    "\n",
    "\n",
    "def animate_solution(maze, path):\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -6: LIGHT_RED, -1: LIGHT_RED};\n",
    "\n",
    "    # Size of the maze\n",
    "    rows, cols = maze.shape\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows))\n",
    "\n",
    "    # Remove the axis ticks and add title title\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('Policy simulation')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Give a color to each cell\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)];\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows))\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(cellText=None,\n",
    "                     cellColours=colored_maze,\n",
    "                     cellLoc='center',\n",
    "                     loc=(0, 0),\n",
    "                     edges='closed')\n",
    "    # Modify the hight and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0 / rows)\n",
    "        cell.set_width(1.0 / cols)\n",
    "\n",
    "    # Update the color at each frame\n",
    "    for i in range(len(path)):\n",
    "        grid.get_celld()[(path[i])].set_facecolor(LIGHT_ORANGE)\n",
    "        grid.get_celld()[(path[i])].get_text().set_text('Player')\n",
    "        if i > 0:\n",
    "            if path[i] == path[i - 1]:\n",
    "                grid.get_celld()[(path[i])].set_facecolor(LIGHT_GREEN)\n",
    "                grid.get_celld()[(path[i])].get_text().set_text('Player is out')\n",
    "            else:\n",
    "                grid.get_celld()[(path[i - 1])].set_facecolor(col_map[maze[path[i - 1]]])\n",
    "                grid.get_celld()[(path[i - 1])].get_text().set_text('')\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(1)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze(maze)\n",
    "horizon = 20\n",
    "V, policy = dynamic_programming(env,horizon)\n",
    "method = 'DynProg'\n",
    "start  = (0,0)\n",
    "path = env.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-135-81d833e348df>\u001b[0m(122)\u001b[0;36m__move_minotaur\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    120 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m__move_minotaur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    121 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 122 \u001b[0;31m        \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    123 \u001b[0;31m        \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    124 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> state\n",
      "37\n",
      "ipdb> self.states[state][0]\n",
      "6\n",
      "ipdb> self.actions[action][0]\n",
      "*** KeyError: (-1, 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = Maze(maze)\n",
    "horizon = 20\n",
    "V, policy = dynamic_programming(env,horizon)\n",
    "method = 'random_walk'\n",
    "start  = (6,5)\n",
    "path = env.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "method = 'random_walk'\n",
    "\n",
    "def simulate(self, start, policy, method):\n",
    "    path = list()\n",
    "    # Deduce the horizon from the policy shape\n",
    "    horizon = policy.shape[1]\n",
    "    # Initialize current state and time\n",
    "    t = 0;\n",
    "    s = self.map[start]\n",
    "    # Add the starting position in the maze to the path\n",
    "    path.append(start)            \n",
    "    while t < horizon - 1:\n",
    "        # Move to next state given the policy and the current state\n",
    "        \n",
    "        next_s = self.__move_minotaur(random.choice(env.actions))\n",
    "        \n",
    "        # Add the position in the maze corresponding to the next state\n",
    "        # to the path\n",
    "        path.append(self.states[next_s])\n",
    "        # Update time and state for next iteration\n",
    "        t += 1;\n",
    "        s = next_s;\n",
    "\n",
    "    return path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
